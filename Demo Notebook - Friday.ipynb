{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style> body {font-family: \"Roboto Condensed Light\", \"Roboto Condensed\";} \n",
       "    h2 {padding: 10px 12px; background-color: #E64626; position: static; color: #ffffff; font-size: 40px;} .text_cell_render p { font-size: 15px; } .text_cell_render \n",
       "    h1 { font-size: 30px; } h1 {padding: 10px 12px;font-style: bold; background-color: #E64626; color: #ffffff; font-size: 40px;} .text_cell_render \n",
       "    h3 { padding: 10px 12px; background-color: #16bedb; position: static; color: #ffffff; font-size: 20px;} \n",
       "    h4 {padding: 8px 20px; font-family: \"Roboto Condensed Light\"; position: static; font-style: italic; background-color: #57fa05; color: #ffffff; font-size: 18px;  border-radius: 5px;}input[type=submit] {background-color: #E64626; border: solid; border-color: #734036; color: white; padding: 8px 16px; text-decoration: none; margin: 4px 2px; cursor: pointer; border-radius: 20px;}</style>\n",
       "    <script> code_show=true; function code_toggle() {if (code_show){$('div.input').hide();} else {$('div.input').show();} code_show = !code_show} $( document ).ready(code_toggle);</script>\n",
       "    <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Hide/show all code.\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "    <style> body {font-family: \"Roboto Condensed Light\", \"Roboto Condensed\";} \n",
    "    h2 {padding: 10px 12px; background-color: #E64626; position: static; color: #ffffff; font-size: 40px;} .text_cell_render p { font-size: 15px; } .text_cell_render \n",
    "    h1 { font-size: 30px; } h1 {padding: 10px 12px;font-style: bold; background-color: #E64626; color: #ffffff; font-size: 40px;} .text_cell_render \n",
    "    h3 { padding: 10px 12px; background-color: #16bedb; position: static; color: #ffffff; font-size: 20px;} \n",
    "    h4 {padding: 8px 20px; font-family: \"Roboto Condensed Light\"; position: static; font-style: italic; background-color: #57fa05; color: #ffffff; font-size: 18px;  border-radius: 5px;}input[type=submit] {background-color: #E64626; border: solid; border-color: #734036; color: white; padding: 8px 16px; text-decoration: none; margin: 4px 2px; cursor: pointer; border-radius: 20px;}</style>\n",
    "    <script> code_show=true; function code_toggle() {if (code_show){$('div.input').hide();} else {$('div.input').show();} code_show = !code_show} $( document ).ready(code_toggle);</script>\n",
    "    <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Hide/show all code.\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sydney Liveability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group - 5  F10Adv-01 : Pratul Singh Raghava & Amanda Walpitage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains the code and processes followed to calculate a **Liveability score** for each SA2 Area in Greater Sydney and further analysis of Inner Sydney suburbs through the integration of additional datasets. The results of this analysis have further been compared to parameters like Median Rent and Median Income of each neighbourhood in an attempt to figure out their correlation with the analysis results. This notebook also contains the analysis by an Unsupervised Machine Learning Algorithm and the comparison of its results with the tradional methods used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with the analysis, we must first import all the necessary modules required to perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Spatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the spatial data from .shp files and store them in GeoPandas DataFrames, which are just like Pandas DataFrames, but with support for Geo-Spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ABS SA2 Shape Data\n",
    "sa2_data = gpd.read_file(\"SA2_2016_AUST/SA2_2016_AUST.shp\")\n",
    "\n",
    "# Importing the shape data of theft ’hotspots’ in NSW \n",
    "break_and_enter = gpd.read_file(\"break_and_enter/BreakEnterDwelling_JanToDec2021.shp\")\n",
    "\n",
    "# Importing the shape data for future Government schools catchments\n",
    "school_catchments_future = gpd.read_file(\"school_catchments/catchments_future.shp\")\n",
    "\n",
    "# Importing the shape data for primary Government schools catchments\n",
    "school_catchments_primary = gpd.read_file(\"school_catchments/catchments_primary.shp\")\n",
    "\n",
    "# Importing the shape data for secondary Government schools catchments\n",
    "school_catchments_secondary = gpd.read_file(\"school_catchments/catchments_secondary.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Non-Spatial CSV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the non-spatial data from .csv files and store them in Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Census data on neighbourhoods(SA2-level areas)\n",
    "neighbourhoods = pd.read_csv('Neighbourhoods.csv')\n",
    "\n",
    "# Importing the Business statistics per SA2-area\n",
    "BusinessStats = pd.read_csv('BusinessStats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Spatial GeoJSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we import the spatial data from the additional datasets\n",
    "[Playgrounds](https://data.cityofsydney.nsw.gov.au/datasets/cityofsydney::playgrounds/about)\n",
    "&\n",
    "[Lights](https://data.cityofsydney.nsw.gov.au/datasets/cityofsydney::lights/about)\n",
    "downloaded from the [\n",
    "City of Sydney Data hub](https://data.cityofsydney.nsw.gov.au/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Playgrounds_json = json.load(open('Playgrounds.geojson'))\n",
    "\n",
    "Playgrounds_DataFrame = pd.DataFrame(Playgrounds_json[\"features\"])\n",
    "Playgrounds_pandas_data=[]\n",
    "for index, row in Playgrounds_DataFrame.iterrows():\n",
    "    append_row = [row[\"geometry\"][\"coordinates\"][0],row[\"geometry\"][\"coordinates\"][1],row[\"properties\"][\"Name\"]]\n",
    "    Playgrounds_pandas_data.append(append_row)\n",
    "    \n",
    "Playgrounds = pd.DataFrame(Playgrounds_pandas_data, columns = ['X', 'Y','Name'])\n",
    "Playgrounds['geom'] = gpd.points_from_xy(Playgrounds.X, Playgrounds.Y)  # creating the geometry column\n",
    "Playgrounds = Playgrounds.drop(columns=['X', 'Y'])  # removing the old latitude/longitude fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lights_json = json.load(open('Lights.geojson'))\n",
    "Lights_DataFrame = pd.DataFrame(Lights_json[\"features\"])\n",
    "Lights_pandas_data=[]\n",
    "for index, row in Lights_DataFrame.iterrows():\n",
    "    append_row = [row[\"geometry\"][\"coordinates\"][0],row[\"geometry\"][\"coordinates\"][1],row[\"properties\"][\"Location\"]]\n",
    "    Lights_pandas_data.append(append_row)\n",
    "    \n",
    "Lights = pd.DataFrame(Lights_pandas_data, columns = ['X', 'Y','Location'])\n",
    "Lights['geom'] = gpd.points_from_xy(Lights.X, Lights.Y)  # creating the geometry column\n",
    "Lights = Lights.drop(columns=['X', 'Y'])  # removing the old latitude/longitude fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the **sa2_data** dataset by dropping those rows where either the locations are :\n",
    "\n",
    "a) `No usual address` - people with no fixed place of abode, or \n",
    "\n",
    "b) `Migratory - Offshore - Shipping` - people who are in transit (Migratory) or on oil rigs and drilling platforms (Offshore) or on board vessels (Shipping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_data = sa2_data.drop(sa2_data[(sa2_data['SA4_NAME16'].str.contains(\"No usual address\", case=False)) |\n",
    "                                  (sa2_data['SA4_NAME16'].str.contains(\"Migratory - Offshore - Shipping\", case=False)) |\n",
    "                         sa2_data['SA3_NAME16'].str.contains(\"No usual address\", case=False) |\n",
    "                         sa2_data['SA3_NAME16'].str.contains(\"Migratory - Offshore - Shipping\", case=False) |\n",
    "                         sa2_data['SA2_NAME16'].str.contains(\"No usual address\", case=False) | \n",
    "                         sa2_data['SA2_NAME16'].str.contains(\"Migratory - Offshore - Shipping\", case=False) | \n",
    "                         sa2_data['GCC_NAME16'].str.contains(\"No usual address\", case=False) | \n",
    "                         sa2_data['GCC_NAME16'].str.contains(\"Migratory - Offshore - Shipping\", case=False) |\n",
    "                         sa2_data['GCC_NAME16'].str.contains(\"Rest of\", case=False)].index\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Task-2 analysis, we filter our data to only contain rows with their region as **'Greater Sydney'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_data = sa2_data[sa2_data['GCC_NAME16']=='Greater Sydney'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we clean the **neighbourhoods** dataset and get rid of the rows where the young population value is zero or null, to avoid ZeroDivisionError and other potential errors later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = neighbourhoods.drop(neighbourhoods[(neighbourhoods['population'].isnull()) | \n",
    "                                      (neighbourhoods['0-4'].isnull()) | \n",
    "                                      (neighbourhoods['5-9'].isnull()) | \n",
    "                                      (neighbourhoods['10-14'].isnull() )| \n",
    "                                      (neighbourhoods['15-19'].isnull())].index)\n",
    "neighbourhoods = neighbourhoods.drop(neighbourhoods[(neighbourhoods['0-4']+neighbourhoods['5-9']+neighbourhoods['10-14']+neighbourhoods['15-19'])<=0].index)\n",
    "\n",
    "# neighbourhoods = neighbourhoods.drop(neighbourhoods[neighbourhoods['population']<=0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the `population` column in the **neighbourhoods** dataset contains values stored as String datatypes. Since we require the population values to be of integer datatypes for our analysis, we iterate through these values and explicitly convert them from Strings to Integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in neighbourhoods.iterrows():\n",
    "    old_value = row['population']\n",
    "    old_value_split = str(row['population']).split(\",\")\n",
    "    new_value = \"\"\n",
    "    for i in old_value_split :\n",
    "        new_value = new_value+i\n",
    "    neighbourhoods['population'] = neighbourhoods['population'].replace([old_value],int(new_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we create a helper function to establish a connection to the servers. The `Credentials.json` file should be stored in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import json\n",
    "\n",
    "credentials = \"Credentials.json\"\n",
    "\n",
    "def pgconnect(credential_filepath, db_schema=\"public\"):\n",
    "    with open(credential_filepath) as f:\n",
    "        db_conn_dict = json.load(f)\n",
    "        host       = db_conn_dict['host']\n",
    "        db_user    = db_conn_dict['user']\n",
    "        db_pw      = db_conn_dict['password']\n",
    "        default_db = db_conn_dict['user']\n",
    "        try:\n",
    "            db = create_engine('postgresql+psycopg2://'+db_user+':'+db_pw+'@'+host+'/'+default_db, echo=False)\n",
    "            conn = db.connect()\n",
    "            print('Connected successfully.')\n",
    "        except Exception as e:\n",
    "            print(\"Unable to connect to the database.\")\n",
    "            print(e)\n",
    "            db, conn = None, None\n",
    "        return db,conn\n",
    "\n",
    "def query(conn, sqlcmd, args=None, df=True):\n",
    "    result = pd.DataFrame() if df else None\n",
    "    try:\n",
    "        if df:\n",
    "            result = pd.read_sql_query(sqlcmd, conn, params=args)\n",
    "        else:\n",
    "            result = conn.execute(sqlcmd, args).fetchall()\n",
    "            result = result[0] if len(result) == 1 else result\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered: \", e, sep='\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code successfully connects to the PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully.\n"
     ]
    }
   ],
   "source": [
    "db, conn = pgconnect(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the below query confirms that POSTGIS which is used for geographical operations is correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postgis_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0 USE_GEOS=1 USE_PROJ=1 USE_STATS=1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         postgis_version\n",
       "0  3.0 USE_GEOS=1 USE_PROJ=1 USE_STATS=1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(conn, \"select PostGIS_Version()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRID Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the spatial data types from GeoPandas are the same as those expected by PostGIS requires conversion to the **Well-Known Text (WKT)** format, as an intermediate step. This can be done using the `geoalchemy2` library, to convert from the `shapely` types in GeoPandas to the WKT format in PostGIS.\n",
    "\n",
    "We'll also be sure to specify the **Spatial Reference Identifier (SRID)** - in this case 4326, to represent the [WGS84 world geodetic coordinate system](https://en.wikipedia.org/wiki/World_Geodetic_System) used by our datasets. The following code simply converts the 'geom' column of the **Lights** & **Playgrounds** dataframe accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "srid = 4326\n",
    "Playgrounds['geom'] = Playgrounds['geom'].apply(lambda x: WKTElement(x.wkt, srid=srid))\n",
    "Lights['geom'] = Lights['geom'].apply(lambda x: WKTElement(x.wkt, srid=srid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the polygons in our **sa2_data**, **break_and_enter**, **school_catchments_primary** , **school_catchments_secondary** & **school_catchments_future** dataframes requires more work. We'll first ensure they're all represented as multipolygons (of which polygons are a subset), and then conduct the same WKT conversion, all using a simple helper function.Then we converts the 'geom' column of the  dataframe accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wkt_element(geom, srid):\n",
    "    if geom!=None :\n",
    "        if geom.geom_type == 'Polygon':\n",
    "            geom = MultiPolygon([geom])\n",
    "        return WKTElement(geom.wkt, srid)\n",
    "\n",
    "sa2_data_og = sa2_data.copy()  # creating a copy of the original for later\n",
    "sa2_data['geom'] = sa2_data['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))  # applying the function\n",
    "sa2_data = sa2_data.drop(columns=\"geometry\")  # deleting the old copy\n",
    "\n",
    "break_and_enter['geom'] = break_and_enter['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))  # applying the function\n",
    "break_and_enter = break_and_enter.drop(columns=\"geometry\")  # deleting the old copy\n",
    "\n",
    "school_catchments_future['geom'] = school_catchments_future['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))  # applying the function\n",
    "school_catchments_future = school_catchments_future.drop(columns=\"geometry\")  # deleting the old copy\n",
    "\n",
    "school_catchments_primary['geom'] = school_catchments_primary['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))  # applying the function\n",
    "school_catchments_primary = school_catchments_primary.drop(columns=\"geometry\")  # deleting the old copy\n",
    "\n",
    "school_catchments_secondary['geom'] = school_catchments_secondary['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))  # applying the function\n",
    "school_catchments_secondary = school_catchments_secondary.drop(columns=\"geometry\")  # deleting the old copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce our dataframes only to contain the variables we are interested in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_data = sa2_data[['SA2_MAIN16', 'SA2_NAME16','SA3_NAME16','AREASQKM16','geom']].copy()\n",
    "break_and_enter = break_and_enter[['OBJECTID', 'Density','ORIG_FID','Shape_Area','geom']].copy()\n",
    "school_catchments_future = school_catchments_future[['USE_ID','CATCH_TYPE', 'USE_DESC','geom']].copy()\n",
    "school_catchments_primary = school_catchments_primary[['USE_ID', 'USE_DESC','geom']].copy()\n",
    "school_catchments_secondary = school_catchments_secondary[['USE_ID', 'USE_DESC','geom']].copy()\n",
    "neighbourhoods = neighbourhoods[['area_id','land_area','population', 'number_of_dwellings','number_of_businesses','median_annual_household_income','avg_monthly_rent','0-4','5-9','10-14','15-19']].copy()\n",
    "BusinessStats = BusinessStats[['area_id','number_of_businesses','accommodation_and_food_services','retail_trade','agriculture_forestry_and_fishing','health_care_and_social_assistance','public_administration_and_safety','transport_postal_and_warehousing']].copy()\n",
    "Lights = Lights[['Location', 'geom']].copy()\n",
    "Playgrounds = Playgrounds[['Name', 'geom']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQL Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create new tables in our schema so we can insert data into them to conduct our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f2b2fbcb890>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Table for sa2_data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS sa2_data;\n",
    "CREATE TABLE sa2_data (\n",
    "    \"SA2_MAIN16\" INTEGER,\n",
    "    \"SA2_NAME16\" VARCHAR(80),\n",
    "    \"SA3_NAME16\" VARCHAR(80),\n",
    "    \"AREASQKM16\" NUMERIC,\n",
    "    \"geom\" GEOMETRY(MULTIPOLYGON,4326)\n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for neighbourhood Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS neighbourhood;\n",
    "CREATE TABLE neighbourhood (\n",
    "    \"area_id\" INTEGER,\n",
    "    \"land_area\" NUMERIC,\n",
    "    \"population\" INTEGER,\n",
    "    \"number_of_dwellings\" VARCHAR(80),\n",
    "    \"number_of_businesses\" INTEGER,\n",
    "    \"median_annual_household_income\" NUMERIC,\n",
    "    \"avg_monthly_rent\" INTEGER,\n",
    "    \"0-4\" INTEGER,\n",
    "    \"5-9\" INTEGER,\n",
    "    \"10-14\" INTEGER,\n",
    "    \"15-19\" INTEGER\n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for break and enter Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS break_and_enter;\n",
    "CREATE TABLE break_and_enter (\n",
    "    \"OBJECTID\" INTEGER,\n",
    "    \"Density\" VARCHAR(80),\n",
    "    \"ORIG_FID\" INTEGER,\n",
    "    \"Shape_Area\" NUMERIC,\n",
    "    \"geom\" GEOMETRY(MULTIPOLYGON,4326)\n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for school_catchments_future Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS school_catchments_future;\n",
    "CREATE TABLE school_catchments_future (\n",
    "    \"USE_ID\" INTEGER, \n",
    "    \"CATCH_TYPE\" VARCHAR(80), \n",
    "    \"USE_DESC\" VARCHAR(80),\n",
    "    \"geom\" GEOMETRY(MULTIPOLYGON,4326)\n",
    "    \n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for school_catchments_primary Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS school_catchments_primary;\n",
    "CREATE TABLE school_catchments_primary (\n",
    "    \"USE_ID\" INTEGER,\n",
    "    \"USE_DESC\" VARCHAR(80),\n",
    "    \"geom\" GEOMETRY(MULTIPOLYGON,4326)\n",
    "    \n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for school_catchments_secondary Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS school_catchments_secondary;\n",
    "CREATE TABLE school_catchments_secondary (\n",
    "    \"USE_ID\" INTEGER,\n",
    "    \"USE_DESC\" VARCHAR(80),\n",
    "    \"geom\" GEOMETRY(MULTIPOLYGON,4326)\n",
    "    \n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# SQL Table for BusinessStats Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS businessstats;\n",
    "CREATE TABLE businessstats (\n",
    "    \"area_id\" INTEGER,\n",
    "    \"number_of_businesses\" INTEGER,\n",
    "    \"accommodation_and_food_services\" INTEGER,\n",
    "    \"retail_trade\" INTEGER,\n",
    "    \"agriculture_forestry_and_fishing\" INTEGER,\n",
    "    \"health_care_and_social_assistance\" INTEGER,\n",
    "    \"public_administration_and_safety\" INTEGER,\n",
    "    \"transport_postal_and_warehousing\" INTEGER\n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for Playgrounds Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS playgrounds;\n",
    "CREATE TABLE playgrounds (\n",
    "   \"Name\" VARCHAR(200),\n",
    "   geom GEOMETRY(POINT, 4326)\n",
    ");\"\"\"\n",
    ")\n",
    "\n",
    "# SQL Table for Lights Data\n",
    "conn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS lights;\n",
    "CREATE TABLE lights (\n",
    "   \"Location\" VARCHAR(200),\n",
    "   geom GEOMETRY(POINT, 4326)\n",
    ");\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the below code to confirm that our SQL tables have been successfully created in the public schema of our PostgreSQL server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial_ref_sys\n",
      "public.BusinessStats\n",
      "BusinessStats\n",
      "Lights\n",
      "Playgrounds\n",
      "cities\n",
      "world\n",
      "school_catchments_combined\n",
      "results\n",
      "task3_data\n",
      "lights_database\n",
      "playgrounds_database\n",
      "task3_results\n",
      "sa2_data\n",
      "neighbourhood\n",
      "break_and_enter\n",
      "school_catchments_future\n",
      "school_catchments_primary\n",
      "school_catchments_secondary\n",
      "businessstats\n",
      "playgrounds\n",
      "lights\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "for table in inspect(db).get_table_names(schema='public') :\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the data from the pandas DataFrames into their respective SQL tables. We specifically define the new datatype of our `geom` columns that is `Multipolygon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the school datasets into SQL tables\n",
    "school_catchments_future.to_sql(\"school_catchments_future\", conn, if_exists='append', index=False, dtype={'geom': Geometry('MULTIPOLYGON', srid)})\n",
    "school_catchments_primary.to_sql(\"school_catchments_primary\", conn, if_exists='append', index=False, dtype={'geom': Geometry('MULTIPOLYGON', srid)})\n",
    "school_catchments_secondary.to_sql(\"school_catchments_secondary\", conn, if_exists='append', index=False, dtype={'geom': Geometry('MULTIPOLYGON', srid)})\n",
    "\n",
    "# Inserting the additional datasets into SQL tables\n",
    "Lights.to_sql('lights', conn, if_exists='append', index=False, dtype={'geom': Geometry('POINT', srid)})\n",
    "Playgrounds.to_sql('playgrounds', conn, if_exists='append', index=False, dtype={'geom': Geometry('POINT', srid)})\n",
    "\n",
    "# Inserting the other datasets into SQL tables\n",
    "sa2_data.to_sql('sa2_data', conn, if_exists='append', index=False, dtype={'geom': Geometry('MULTIPOLYGON', srid)})\n",
    "neighbourhoods.to_sql('neighbourhood', conn, if_exists='append', index=False)\n",
    "BusinessStats.to_sql('businessstats', conn, if_exists='append', index=False)\n",
    "break_and_enter.to_sql(\"break_and_enter\", conn, if_exists='append', index=False, dtype={'geom': Geometry('MULTIPOLYGON', srid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganising SQL Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the 3 different school catchment tables into a new table for the purpose of practicality. We can now get rid of the earlier tables since they are not of any further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered: \n",
      "This result object does not return rows. It has been closed automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql=\"\"\"\n",
    "DROP TABLE IF EXISTS school_catchments_combined;\n",
    "CREATE TABLE school_catchments_combined (\n",
    "    \"USE_ID\" INTEGER,\n",
    "    \"USE_DESC\" VARCHAR(80),\n",
    "    \"geom\" GEOMETRY(MULTIPOLYGON,4326)\n",
    ");\n",
    "\n",
    "INSERT INTO school_catchments_combined(\"USE_ID\",\"USE_DESC\",\"geom\")\n",
    "    SELECT * FROM school_catchments_primary\n",
    "    UNION ALL\n",
    "    SELECT * FROM school_catchments_secondary\n",
    "    UNION ALL\n",
    "    SELECT \"USE_ID\",\"USE_DESC\",\"geom\" FROM school_catchments_future\n",
    "    ;\n",
    "\n",
    "DROP TABLE IF EXISTS school_catchments_primary;\n",
    "DROP TABLE IF EXISTS school_catchments_secondary;\n",
    "DROP TABLE IF EXISTS school_catchments_future;\n",
    "\"\"\"\n",
    "\n",
    "query(conn,sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create indices on our database tables to improve the run time and reduce the cost of executing queries.\n",
    "\n",
    "As primary keys of our tables are indexes by default we don't need explicit indexes on those. As our data analysis summarises results by suburbs it is meaningful to create an index on \"SA2_NAME16\" in our sa2_data table (Note that the primary key of this table is \"SA2_MAIN16\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered: \n",
      "Could not parse rfc1738 URL from string '\n",
      "CREATE INDEX sa2_name16_idx ON sa2_data(\"SA2_NAME16\");\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an index on sa2_name in sa2_data set\n",
    "sql = \"\"\"\n",
    "CREATE INDEX sa2_name16_idx ON sa2_data(\"SA2_NAME16\");\n",
    "\"\"\"\n",
    "query(sql,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial indices reduce the time and cost of running spatial joins in our tables. We created spatial indexes on the geometry column of our shape datasets, because 'geom' column is used when joining data from other tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered: \n",
      "Could not parse rfc1738 URL from string '\n",
      "CREATE INDEX school_catchments_combined_geom_idx ON school_catchments_combined USING GIST (geom);\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# school_catchments_combined\n",
    "sql = \"\"\"\n",
    "CREATE INDEX school_catchments_combined_geom_idx ON school_catchments_combined USING GIST (geom);\n",
    "\"\"\"\n",
    "query(sql,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered: \n",
      "Could not parse rfc1738 URL from string '\n",
      "CREATE INDEX break_and_enter_geom_idx ON break_and_enter USING GIST (geom);\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break_and_enter\n",
    "sql = \"\"\"\n",
    "CREATE INDEX break_and_enter_geom_idx ON break_and_enter USING GIST (geom);\n",
    "\"\"\"\n",
    "query(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Main Analysis for Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis we join our sa2_data, businessstats and neighbourhood datasets on the basis of the area_id(or \"SA2_MAIN16\").This gives an idea of the businesses and neighbourhood statistics per each suburb.\n",
    "Break_and_enter and school_catchment_combined datasets are combined together with sa2_data considering their geometric location(\"geom\"). We use the POSTGIS function [ST_Intersects](https://postgis.net/docs/ST_Intersects.html) to determine which areas of sa2 suburbs have the school catchment areas and break and enter hotspots.\n",
    "\n",
    "In order to compute the liveability score of Greater Sydney Area, following formula is used.\n",
    "\n",
    "            score = S(Zschool + Zaccomm + Zretail - Zcrime + Zhealth)\n",
    "\n",
    "where S = sigmoid function, Z = normal zscore\n",
    "\n",
    "To calculate the liveability of the Greater Sydney Area we first calculate the measures according to the given descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Measure | Description |\n",
    "| -- | -- |\n",
    "| school | number of schools catchment areas per 1000 ’young people’ |\n",
    "| accom | number of accommodation and food services per 1000 people |\n",
    "| retail| number of retail services per 1000 people |\n",
    "| crime | sum of hotspot areas divided by total area |\n",
    "| health | number of health services per 1000 people |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use sub queries to compute the mean, standard deviation of each factor that affects our sigmoid score. They are referenced later inside the formulas that calculate zscores and sigmoid score in the main query.\n",
    "Finally we group the results by suburb name(\"SA2_NAME16\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "DROP TABLE IF EXISTS results;\n",
    "CREATE TABLE results\n",
    "AS\n",
    "\n",
    "with Health AS \n",
    "(SELECT avg(1000.0*bs.\"health_care_and_social_assistance\"/n.\"population\") AS mean,\n",
    "stddev(1000.0*bs.\"health_care_and_social_assistance\"/n.\"population\") AS sd\n",
    "FROM businessstats bs, neighbourhood n\n",
    "WHERE bs.\"area_id\" = n.\"area_id\"),\n",
    "\n",
    "Retail AS\n",
    "(SELECT avg(1000.0*bs.\"retail_trade\"/n.\"population\") AS mean,\n",
    "stddev(1000.0*bs.\"retail_trade\"/n.\"population\") AS sd\n",
    "FROM businessstats bs, neighbourhood n\n",
    "WHERE bs.\"area_id\" = n.\"area_id\"),\n",
    "\n",
    "Accom AS\n",
    "(SELECT avg(1000.0*bs.\"accommodation_and_food_services\"/n.\"population\") AS mean,\n",
    "stddev(1000.0*bs.\"accommodation_and_food_services\"/n.\"population\") AS sd\n",
    "FROM businessstats bs, neighbourhood n\n",
    "WHERE bs.\"area_id\" = n.\"area_id\"),\n",
    "\n",
    "Crime AS \n",
    "(SELECT AVG(q1) AS mean,\n",
    "stddev(q1) AS sd\n",
    "FROM ( SELECT AVG(be.\"ORIG_FID\"*be.\"Shape_Area\"/s.\"AREASQKM16\")as q1\n",
    "        FROM break_and_enter be join sa2_data s ON\n",
    "        ST_Intersects(s.geom,be.geom)\n",
    "        GROUP BY s.\"SA2_NAME16\") a ),\n",
    "        \n",
    "Schools AS \n",
    "(SELECT AVG(Schools) AS mean,\n",
    "stddev(Schools) as sd \n",
    "FROM (SELECT 1000.0*COUNT(*)/(AVG(n.\"0-4\")+AVG(n.\"5-9\")+AVG(n.\"10-14\")+AVG(n.\"15-19\")) AS Schools\n",
    "    FROM school_catchments_combined sc, sa2_data s, neighbourhood n\n",
    "    WHERE ST_Intersects(s.geom,sc.geom)\n",
    "    AND s.\"SA2_MAIN16\" = n.\"area_id\"\n",
    "    GROUP BY s.\"SA2_NAME16\" ) b)\n",
    "\n",
    "SELECT s.\"SA2_NAME16\",\n",
    "\n",
    "abs((1000.0*AVG(bs.\"health_care_and_social_assistance\")/AVG(n.\"population\"))-AVG(Health.mean))/AVG(Health.sd) AS HZ,\n",
    "\n",
    "abs((1000.0*AVG(bs.\"retail_trade\")/AVG(n.\"population\"))-AVG(Retail.mean))/AVG(Retail.sd) AS RZ,\n",
    "\n",
    "abs((1000.0*AVG(bs.\"accommodation_and_food_services\")/AVG(n.\"population\"))-AVG(Accom.mean))/AVG(Accom.sd) AS AZ,\n",
    "\n",
    "abs((AVG(be.\"ORIG_FID\"*be.\"Shape_Area\"/s.\"AREASQKM16\")-AVG(Crime.mean))/AVG(Crime.sd)) AS CZ,\n",
    "\n",
    "abs((1000.0*COUNT(*)/(AVG(n.\"0-4\")+AVG(n.\"5-9\")+AVG(n.\"10-14\")+AVG(n.\"15-19\"))-AVG(Schools.mean))/AVG(Schools.sd)) AS SZ,\n",
    "\n",
    "1/(1+(exp(-((abs((1000.0*AVG(bs.\"health_care_and_social_assistance\")/AVG(n.\"population\"))-AVG(Health.mean))/AVG(Health.sd))\n",
    "            +(abs((1000.0*AVG(bs.\"retail_trade\")/AVG(n.\"population\"))-AVG(Retail.mean))/AVG(Retail.sd))\n",
    "            +(abs((1000.0*AVG(bs.\"accommodation_and_food_services\")/AVG(n.\"population\"))-AVG(Accom.mean))/AVG(Accom.sd))\n",
    "            -(abs((AVG(be.\"ORIG_FID\"*be.\"Shape_Area\"/s.\"AREASQKM16\")-AVG(Crime.mean))/AVG(Crime.sd)))\n",
    "            +(abs((1000.0*COUNT(*)/(AVG(n.\"0-4\")+AVG(n.\"5-9\")+AVG(n.\"10-14\")+AVG(n.\"15-19\"))-AVG(Schools.mean))/AVG(Schools.sd))))))) AS Sigmoid\n",
    "\n",
    "FROM sa2_data s, businessstats bs, break_and_enter be, neighbourhood n,school_catchments_combined sc, Health, Retail, Accom, crime, Schools\n",
    "WHERE s.\"SA2_MAIN16\" = bs.\"area_id\"\n",
    "AND bs.\"area_id\" = n.\"area_id\"\n",
    "AND s.\"SA2_MAIN16\" = n.\"area_id\"\n",
    "AND ST_Intersects(s.geom,be.geom)\n",
    "AND ST_Intersects(s.geom,sc.geom)\n",
    "GROUP BY s.\"SA2_NAME16\"\n",
    "ORDER BY sigmoid desc\n",
    ";\n",
    "\"\"\"\n",
    "# Our Results\n",
    "# results_df = query(conn,sql)\n",
    "# results_df\n",
    "\n",
    "# The common data between our result and sa2_data\n",
    "# gdf = gpd.GeoDataFrame(results)\n",
    "# gdf2 = gpd.read_file(\"SA2_2016_AUST/SA2_2016_AUST.shp\")\n",
    "# mg = gdf.reset_index().merge(gdf2, left_on = 'SA2_NAME16', right_on = 'SA2_NAME16') # Good\n",
    "# mg \n",
    "\n",
    "# Greater Sydney Map\n",
    "# gdf3 = gdf2[gdf2['GCC_NAME16']=='Greater Sydney'] \n",
    "# gdf3.plot(figsize=(10,6))\n",
    "# gdf3\n",
    "\n",
    "# Missing Rows Area\n",
    "# df = pd.concat([mg.drop(columns=['index', 'hz','rz','az','cz','sz','sigmoid']), gdf3])\n",
    "# gdf4 = df.drop_duplicates(keep=False)\n",
    "# gdf4\n",
    "# gdf4.plot()\n",
    "\n",
    "\n",
    "\n",
    "# Correctly printing the Coloured Map\n",
    "query(conn,sql)\n",
    "results = query(conn,\"SELECT * FROM results\")\n",
    "gdf = gpd.GeoDataFrame(results)\n",
    "gdf2 = gpd.read_file(\"SA2_2016_AUST/SA2_2016_AUST.shp\")\n",
    "mg = gdf.reset_index().merge(gdf2, left_on = 'SA2_NAME16', right_on = 'SA2_NAME16') \n",
    "gdf3 = gdf2[gdf2['GCC_NAME16']=='Greater Sydney']\n",
    "df = pd.concat([mg.drop(columns=['index', 'hz','rz','az','cz','sz','sigmoid']), gdf3])\n",
    "gdf4 = df.drop_duplicates(keep=False)\n",
    "gdf = gpd.GeoDataFrame(results)\n",
    "sa2_areas = gpd.read_file('SA2_2016_AUST/SA2_2016_AUST.shp')\n",
    "merged = gdf.set_index('SA2_NAME16').join(sa2_areas.set_index('SA2_NAME16'))\n",
    "# merged\n",
    "gdf = gpd.GeoDataFrame(results)\n",
    "merged2 = pd.concat([merged, gdf4], axis=0)\n",
    "# # merged2['geometry'] = merged2['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))  # applying the function\n",
    "# # merged2\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "ax.set_title('Greater Sydney Liveability Analysis', fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "# merged2.drop\n",
    "merged2.drop(merged2.index[0]).plot(column='sigmoid',\n",
    "            cmap='RdYlGn',\n",
    "            linewidth=0.9,\n",
    "            ax=ax,\n",
    "            edgecolor='1',\n",
    "            legend=True, \n",
    "            legend_kwds={\"label\": \"Sigmoid Score\"},\n",
    "            missing_kwds={\"color\": \"lightgrey\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Top 5 places with Highest Liveability Scores are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_Query_for_Best_Places=\"\"\"\n",
    "SELECT * FROM results\n",
    "ORDER BY sigmoid DESC\n",
    "limit(5);\n",
    "\"\"\"\n",
    "query(conn,SQL_Query_for_Best_Places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bottom 5 places with Lowest Liveability Scores are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_Query_for_Worst_Places=\"\"\"\n",
    "SELECT * FROM results\n",
    "ORDER BY sigmoid \n",
    "limit(5);\n",
    "\"\"\"\n",
    "query(conn,SQL_Query_for_Worst_Places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Correlation between the Liveability Score and the Median Rent and Median Income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We join our results table (table containing zscores and sigmoid score),neighbourhood and sa2 data tables. Then we apply the `corr` function function on `sigmoid` and `avg_monthly_rent` columns. Similarly, we perform this to the `sigmoid` and `median_annual_household_income` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_Query_to_create_task3_data = \"\"\"\n",
    "DROP TABLE IF EXISTS task3_data;\n",
    "CREATE TABLE task3_data AS\n",
    "select \n",
    "\"hz\", \n",
    "\"rz\", \n",
    "\"az\", \n",
    "\"cz\", \n",
    "\"sz\", \n",
    "\"sigmoid\",\n",
    "\"area_id\",\n",
    "\"land_area\",\n",
    "\"population\",\n",
    "\"number_of_dwellings\",\n",
    "\"number_of_businesses\",\n",
    "\"median_annual_household_income\",\n",
    "\"avg_monthly_rent\",\n",
    "\"0-4\",\n",
    "\"5-9\",\n",
    "\"10-14\",\n",
    "\"15-19\",\n",
    "\"SA2_MAIN16\",\n",
    "s.\"SA2_NAME16\",\n",
    "\"SA3_NAME16\",\n",
    "\"AREASQKM16\",\n",
    "\"geom\"\n",
    "from results R, neighbourhood N, sa2_data s\n",
    "where s.\"SA2_MAIN16\" = N.\"area_id\"\n",
    "and r.\"SA2_NAME16\"=s.\"SA2_NAME16\"\n",
    "Order by sigmoid desc;\n",
    "\"\"\"\n",
    "query(conn,SQL_Query_to_create_task3_data)\n",
    "# results_table_combined=query(conn,\"SELECT * FROM task3_data\")\n",
    "# results_table_combined['sigmoid'].corr(results_table_combined['avg_monthly_rent']) # -0.14809587628537235\n",
    "# results_table_combined['sigmoid'].corr(results_table_combined['median_annual_household_income']) # -0.1351918056836305\n",
    "\n",
    "SQL_Query_to_find_Correlation = \"\"\" SELECT corr(\"sigmoid\",\"avg_monthly_rent\") AS \"Rent Correlation\",\n",
    "corr(\"sigmoid\",\"median_annual_household_income\") AS \"Income Correlation\" FROM task3_data; \"\"\"\n",
    "query(conn,SQL_Query_to_find_Correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of this analysis points out that there's no strong linear relationship between the liveability score and median rent/income of each neighbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we divert our attention to the City of Sydney within the Greater Sydney Area. Here we focus to analyse the suitablity of each suburb within the city of Sydney for a family of three with a young child. In addition to health, retail , accommodation, school facilities and concerns about theft for this family we identified safety and recreational facilites are important when settling down in the city.\n",
    "\n",
    "Therefore we use the [Lights](https://data.cityofsydney.nsw.gov.au/datasets/cityofsydney::lights/about) dataset which gives information about the lights controlled by Sydney City Council and [Playgrounds](https://data.cityofsydney.nsw.gov.au/datasets/cityofsydney::playgrounds/about) dataset which gives data on the playgrounds in the city obtained from City of Sydney Open Data Hub.\n",
    "\n",
    "The modified formula to calculate the liveability for our stake holder is given below.\n",
    "\n",
    "                        score = S(Z_school + Z_accomm + Z_retail - Z_crime + Z_health + Z_lights + Z_playgrounds)\n",
    "                        \n",
    "In addition to the zscores calculated in task 2 we compute zscores for lights and playgrounds. Higher the number of lights in a suburb higher the safety of the neighbourhood and reduced chances of crime. Also more number of playgrounds implies better recreational facilities. Therefore both these zscores are \"added\" to the liveability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK-3\n",
    "\n",
    "sql=\"\"\"\n",
    "DROP TABLE IF EXISTS Lights_database;\n",
    "CREATE TABLE Lights_database AS\n",
    "\n",
    "WITH Lights_data AS \n",
    "(SELECT avg(q1) AS mean,\n",
    "stddev(q1) AS sd\n",
    "FROM (SELECT COUNT(*) as q1 FROM\n",
    "    task3_data t, lights l\n",
    "    where \"SA3_NAME16\"='Sydney Inner City'\n",
    "    AND ST_Contains(t.geom,l.geom)\n",
    "    GROUP BY \"SA2_NAME16\" ) a)\n",
    "\n",
    "SELECT t.\"SA2_NAME16\",(abs(COUNT(*)-AVG(ld.mean))/AVG(ld.sd)) AS lz\n",
    "FROM task3_data t, lights l, lights_data ld\n",
    "where \"SA3_NAME16\"='Sydney Inner City'\n",
    "AND ST_Contains(t.geom,l.geom)\n",
    "GROUP BY \"SA2_NAME16\"\n",
    ";\n",
    "\"\"\"\n",
    "query(conn,sql)\n",
    "\n",
    "sql=\"\"\"\n",
    "DROP TABLE IF EXISTS Playgrounds_database;\n",
    "CREATE TABLE Playgrounds_database AS\n",
    "\n",
    "WITH Playgrounds_data AS \n",
    "(SELECT avg(q2) AS mean,\n",
    "stddev(q2) AS sd\n",
    "FROM (SELECT COUNT(*) as q2 FROM\n",
    "    task3_data t, Playgrounds p\n",
    "    where \"SA3_NAME16\"='Sydney Inner City'\n",
    "    AND ST_Contains(t.geom,p.geom)\n",
    "    GROUP BY \"SA2_NAME16\" ) a)\n",
    "\n",
    "SELECT t.\"SA2_NAME16\",(abs(COUNT(*)-AVG(pd.mean))/AVG(pd.sd)) AS pz\n",
    "FROM task3_data t, playgrounds p, playgrounds_data pd\n",
    "where \"SA3_NAME16\"='Sydney Inner City'\n",
    "AND ST_Contains(t.geom,p.geom)\n",
    "GROUP BY \"SA2_NAME16\"\n",
    ";\n",
    "\"\"\"\n",
    "query(conn,sql)\n",
    "# query(conn,\"SELECT * FROM Playgrounds_database\")\n",
    "# query(conn,\"SELECT * FROM Lights_database\")\n",
    "\n",
    "sql=\"\"\"\n",
    "DROP TABLE IF EXISTS task3_results;\n",
    "CREATE TABLE task3_results AS\n",
    "\n",
    "SELECT \n",
    "t.\"SA2_NAME16\",\n",
    "1/(1+(exp(-((hz)+(rz)+(az)-(cz)+(sz)+(lz)+(pz))))) AS Sigmoid,\n",
    "t.geom\n",
    "\n",
    "FROM \n",
    "task3_data t, lights_database l, playgrounds_database p\n",
    "WHERE t.\"SA2_NAME16\"=l.\"SA2_NAME16\"\n",
    "AND p.\"SA2_NAME16\"=l.\"SA2_NAME16\"\n",
    "ORDER BY Sigmoid desc\n",
    "\"\"\"\n",
    "\n",
    "# Correctly printing the Coloured Map\n",
    "query(conn,sql)\n",
    "results_task3 = query(conn,\"SELECT * FROM task3_results\")\n",
    "gdf_task3 = gpd.GeoDataFrame(results_task3)\n",
    "# sa2_areas = gpd.read_file('SA2_2016_AUST/SA2_2016_AUST.shp')\n",
    "merged_task3 = gdf_task3.set_index('SA2_NAME16').join(sa2_data_og.set_index('SA2_NAME16'))\n",
    "merged_task3\n",
    "# merged2 = pd.concat([merged, gdf4], axis=0)\n",
    "# merged2\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "ax.set_title('Greater Sydney Liveability Analysis', fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "merged_task3.plot(column='sigmoid',\n",
    "            cmap='RdYlGn',\n",
    "            linewidth=0.9,\n",
    "            ax=ax,\n",
    "            edgecolor='1',\n",
    "            legend=True, \n",
    "            legend_kwds={\"label\": \"Sigmoid Score\"},\n",
    "            missing_kwds={\"color\": \"lightgrey\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Liveability Scores for Inner City Sydney are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_Query_For_Inner_Sydney = \"\"\"\n",
    "SELECT * FROM task3_results\n",
    "ORDER BY sigmoid DESC;\n",
    "\"\"\"\n",
    "query(conn,SQL_Query_For_Inner_Sydney)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now deploy an Unsupervised Machine Learning Algorithm to calculate the Liveability Scores for all SA2 areas in Greater Sydney."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged['cz_neg']=merged['cz']**(-1)\n",
    "# merged\n",
    "\n",
    "# Defining Model\n",
    "model = TSNE(learning_rate=100)\n",
    "\n",
    "# Fitting Model\n",
    "transformed = model.fit_transform(merged[['hz','rz','az','cz_neg','sz']])\n",
    "\n",
    "# Plotting 2d t-Sne\n",
    "x_axis = transformed[:, 0]\n",
    "y_axis = transformed[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "ML_merged=merged.copy()\n",
    "for i in range(len(x_axis)) :\n",
    "    new_row = math.sqrt(x_axis[i]**2 + y_axis[i]**2)\n",
    "    data.append([new_row])\n",
    "\n",
    "ML_df = pd.DataFrame(data,columns=['ML_Score'])\n",
    "# ML_df\n",
    "ML_merged = pd.concat([merged.reset_index(), ML_df], axis=1)\n",
    "ML_merged = pd.concat([ML_merged, gdf4], axis=0)\n",
    "# ML_merged\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "ax.set_title('Greater Sydney Liveability Analysis by ML', fontdict={'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "ML_merged.drop(0).plot(column='ML_Score',\n",
    "            cmap='RdYlGn',\n",
    "            linewidth=0.9,\n",
    "            ax=ax,\n",
    "            edgecolor='1',\n",
    "            legend=True, \n",
    "            legend_kwds={\"label\": \"ML Score\"},\n",
    "            missing_kwds={\"color\": \"lightgrey\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The places with the Highest Liveability Score as per Machine Learning is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_merged.sort_values('ML_Score',ascending=False).drop(0)[['SA2_NAME16','hz','rz','az','cz_neg','sz','sigmoid','ML_Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The places with the Lowest Liveability Score as per Machine Learning is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_merged.sort_values('ML_Score').drop(0)[['SA2_NAME16','hz','rz','az','cz_neg','sz','sigmoid','ML_Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having completed our analysis, we will close our connection to the PostgreSQL server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-ade7356e990b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "db.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
